{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.nb.stop_words import STOP_WORDS\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = Path('model')\n",
    "SAVE_PATH.mkdir(exist_ok=True)\n",
    "DATA_PATH = Path('../data/norec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_names = ['train', 'test', 'dev']\n",
    "subsets = {name: pd.read_pickle(DATA_PATH / f'norsk_kategori_{name}.pkl') for name in subset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = subsets['train'].iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"«Poison». Som alle store artister passer Timberlake på å synliggjøre hvor han kommer fra musikalsk.. Derav denne relativt obskure new jack swing-saken fra Bell Biv DeVoe, gruppen som ble til New Edition og som sådan forløpere til N'Sync.. Fenomenalt frekk låt som skreddersydd for Justin.\""
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         text\n",
       "rating       \n",
       "0        2326\n",
       "1       11597"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>rating</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2326</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11597</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "subsets['train'].groupby(['rating']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=STOP_WORDS, min_df=5, max_df=1000, max_features=10000)\n",
    "vectorizer.fit_transform(subsets['train']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['00', '000', '10', '100', '1000', '1024', '105', '1080', '1080p', '11']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {name: vectorizer.transform(subsets[name]['text']) for name in subset_names}\n",
    "categories = {name: subsets[name]['rating'] for name in subset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.69      0.81      2326\n",
      "           1       0.94      1.00      0.97     11597\n",
      "\n",
      "    accuracy                           0.95     13923\n",
      "   macro avg       0.96      0.84      0.89     13923\n",
      "weighted avg       0.95      0.95      0.94     13923\n",
      "\n",
      "Development metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.50      0.64       230\n",
      "           1       0.93      0.99      0.96      1569\n",
      "\n",
      "    accuracy                           0.93      1799\n",
      "   macro avg       0.91      0.75      0.80      1799\n",
      "weighted avg       0.93      0.93      0.92      1799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boosted_model = xgb.XGBClassifier()\n",
    "boosted_model.fit(texts['train'], categories['train'])\n",
    "print('Training metrics')\n",
    "print(classification_report(categories['train'], boosted_model.predict(texts['train'])))\n",
    "print('Development metrics')\n",
    "print(classification_report(categories['dev'], boosted_model.predict(texts['dev'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.98      0.80      2326\n",
      "           1       1.00      0.90      0.95     11597\n",
      "\n",
      "    accuracy                           0.92     13923\n",
      "   macro avg       0.83      0.94      0.87     13923\n",
      "weighted avg       0.94      0.92      0.92     13923\n",
      "\n",
      "Development metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.79      0.65       230\n",
      "           1       0.97      0.91      0.94      1569\n",
      "\n",
      "    accuracy                           0.89      1799\n",
      "   macro avg       0.76      0.85      0.80      1799\n",
      "weighted avg       0.92      0.89      0.90      1799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boosted_model = xgb.XGBClassifier(scale_pos_weight=0.2)\n",
    "boosted_model.fit(texts['train'], categories['train'])\n",
    "print('Training metrics')\n",
    "print(classification_report(categories['train'], boosted_model.predict(texts['train'])))\n",
    "print('Development metrics')\n",
    "print(classification_report(categories['dev'], boosted_model.predict(texts['dev'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.98      0.80      2326\n",
      "           1       1.00      0.90      0.95     11597\n",
      "\n",
      "    accuracy                           0.92     13923\n",
      "   macro avg       0.83      0.94      0.87     13923\n",
      "weighted avg       0.94      0.92      0.92     13923\n",
      "\n",
      "Development metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.80      0.64       230\n",
      "           1       0.97      0.90      0.93      1569\n",
      "\n",
      "    accuracy                           0.89      1799\n",
      "   macro avg       0.75      0.85      0.79      1799\n",
      "weighted avg       0.91      0.89      0.90      1799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=STOP_WORDS, min_df=5, max_df=1000, max_features=5000)\n",
    "vectorizer.fit_transform(subsets['train']['text'])\n",
    "texts = {name: vectorizer.transform(subsets[name]['text']) for name in subset_names}\n",
    "categories = {name: subsets[name]['rating'] for name in subset_names}\n",
    "boosted_model = xgb.XGBClassifier(scale_pos_weight=0.2)\n",
    "boosted_model.fit(texts['train'], categories['train'])\n",
    "print('Training metrics')\n",
    "print(classification_report(categories['train'], boosted_model.predict(texts['train'])))\n",
    "print('Development metrics')\n",
    "print(classification_report(categories['dev'], boosted_model.predict(texts['dev'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.98      0.81      2326\n",
      "           1       1.00      0.91      0.95     11597\n",
      "\n",
      "    accuracy                           0.92     13923\n",
      "   macro avg       0.84      0.95      0.88     13923\n",
      "weighted avg       0.94      0.92      0.93     13923\n",
      "\n",
      "Development metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.81      0.66       230\n",
      "           1       0.97      0.91      0.94      1569\n",
      "\n",
      "    accuracy                           0.89      1799\n",
      "   macro avg       0.76      0.86      0.80      1799\n",
      "weighted avg       0.92      0.89      0.90      1799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=STOP_WORDS, min_df=5, max_df=1000, max_features=20000)\n",
    "vectorizer.fit_transform(subsets['train']['text'])\n",
    "texts = {name: vectorizer.transform(subsets[name]['text']) for name in subset_names}\n",
    "categories = {name: subsets[name]['rating'] for name in subset_names}\n",
    "boosted_model = xgb.XGBClassifier(scale_pos_weight=0.2)\n",
    "boosted_model.fit(texts['train'], categories['train'])\n",
    "print('Training metrics')\n",
    "print(classification_report(categories['train'], boosted_model.predict(texts['train'])))\n",
    "print('Development metrics')\n",
    "print(classification_report(categories['dev'], boosted_model.predict(texts['dev'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.97      0.77      2326\n",
      "           1       0.99      0.89      0.94     11597\n",
      "\n",
      "    accuracy                           0.90     13923\n",
      "   macro avg       0.81      0.93      0.85     13923\n",
      "weighted avg       0.93      0.90      0.91     13923\n",
      "\n",
      "Development metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.74      0.60       230\n",
      "           1       0.96      0.89      0.93      1569\n",
      "\n",
      "    accuracy                           0.87      1799\n",
      "   macro avg       0.73      0.82      0.76      1799\n",
      "weighted avg       0.90      0.87      0.88      1799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=STOP_WORDS, min_df=5, max_df=1000, max_features=1000)\n",
    "vectorizer.fit_transform(subsets['train']['text'])\n",
    "texts = {name: vectorizer.transform(subsets[name]['text']) for name in subset_names}\n",
    "categories = {name: subsets[name]['rating'] for name in subset_names}\n",
    "boosted_model = xgb.XGBClassifier(scale_pos_weight=0.2)\n",
    "boosted_model.fit(texts['train'], categories['train'])\n",
    "print('Training metrics')\n",
    "print(classification_report(categories['train'], boosted_model.predict(texts['train'])))\n",
    "print('Development metrics')\n",
    "print(classification_report(categories['dev'], boosted_model.predict(texts['dev'])))"
   ]
  },
  {
   "source": [
    "Not much difference in 5000, 10000, 20000 features for XGBoost. With 1000 features however, the result is noticably worse.\n",
    "\n",
    "While it's not overfitting as much as the Logistic Regression, going with 5000 features will make everything faster."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('text-oqa78tdt': venv)",
   "display_name": "Python 3.8.5 64-bit ('text-oqa78tdt': venv)",
   "metadata": {
    "interpreter": {
     "hash": "b79a91171a1b654441da0aa02197604a6e053331b5ce31ae0001fbbaddbfb627"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}