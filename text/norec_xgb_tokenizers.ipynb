{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = Path('model')\n",
    "TOKENIZER_PATH = SAVE_PATH / 'norec_tokenizer_clean.json'\n",
    "MAX_SENTENCE_LENGTH = 5000\n",
    "DATA_PATH = Path('../data/norec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_names = ['train', 'test', 'dev']\n",
    "subsets = {name: pd.read_pickle(DATA_PATH / f'norsk_kategori_{name}.pkl') for name in subset_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = subsets['train'].iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"«Poison». Som alle store artister passer Timberlake på å synliggjøre hvor han kommer fra musikalsk.. Derav denne relativt obskure new jack swing-saken fra Bell Biv DeVoe, gruppen som ble til New Edition og som sådan forløpere til N'Sync.. Fenomenalt frekk låt som skreddersydd for Justin.\""
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         text\n",
       "rating       \n",
       "0        2326\n",
       "1       11597"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>rating</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2326</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11597</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "subsets['train'].groupby(['rating']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "tokenizer.enable_padding(length=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      2326\n",
      "           1       1.00      0.99      1.00     11597\n",
      "\n",
      "    accuracy                           0.99     13923\n",
      "   macro avg       0.98      1.00      0.99     13923\n",
      "weighted avg       0.99      0.99      0.99     13923\n",
      "\n",
      "Development metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.12      0.14       230\n",
      "           1       0.88      0.92      0.90      1569\n",
      "\n",
      "    accuracy                           0.82      1799\n",
      "   macro avg       0.53      0.52      0.52      1799\n",
      "weighted avg       0.79      0.82      0.80      1799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = {name: np.array([encoding.ids if len(encoding.ids) < MAX_SENTENCE_LENGTH else encoding.ids[:MAX_SENTENCE_LENGTH] for encoding in tokenizer.encode_batch(subsets[name]['text'])]) for name in subset_names}\n",
    "categories = {name: subsets[name]['rating'] for name in subset_names}\n",
    "boosted_model = xgb.XGBClassifier(scale_pos_weight=0.2)\n",
    "boosted_model.fit(texts['train'], categories['train'])\n",
    "print('Training metrics')\n",
    "print(classification_report(categories['train'], boosted_model.predict(texts['train'])))\n",
    "print('Development metrics')\n",
    "print(classification_report(categories['dev'], boosted_model.predict(texts['dev'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "def norec_tokenizer(text):\n",
    "    encoding = text_tokenizer.encode(text)\n",
    "    return encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.99      0.83      2326\n",
      "           1       1.00      0.92      0.96     11597\n",
      "\n",
      "    accuracy                           0.93     13923\n",
      "   macro avg       0.85      0.95      0.89     13923\n",
      "weighted avg       0.95      0.93      0.93     13923\n",
      "\n",
      "Development metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.80      0.68       230\n",
      "           1       0.97      0.92      0.94      1569\n",
      "\n",
      "    accuracy                           0.90      1799\n",
      "   macro avg       0.78      0.86      0.81      1799\n",
      "weighted avg       0.92      0.90      0.91      1799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=norec_tokenizer, stop_words=None, min_df=5, max_df=1000, max_features=10000)\n",
    "vectorizer.fit_transform(subsets['train']['text'])\n",
    "texts = {name: vectorizer.transform(subsets[name]['text']) for name in subset_names}\n",
    "categories = {name: subsets[name]['rating'] for name in subset_names}\n",
    "boosted_model = xgb.XGBClassifier(scale_pos_weight=0.2)\n",
    "boosted_model.fit(texts['train'], categories['train'])\n",
    "print('Training metrics')\n",
    "print(classification_report(categories['train'], boosted_model.predict(texts['train'])))\n",
    "print('Development metrics')\n",
    "print(classification_report(categories['dev'], boosted_model.predict(texts['dev'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69      2326\n",
      "           1       1.00      0.82      0.90     11597\n",
      "\n",
      "    accuracy                           0.85     13923\n",
      "   macro avg       0.77      0.91      0.80     13923\n",
      "weighted avg       0.92      0.85      0.87     13923\n",
      "\n",
      "Development metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.88      0.58       230\n",
      "           1       0.98      0.83      0.90      1569\n",
      "\n",
      "    accuracy                           0.84      1799\n",
      "   macro avg       0.71      0.86      0.74      1799\n",
      "weighted avg       0.91      0.84      0.86      1799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=norec_tokenizer, stop_words=None, min_df=5, max_df=1000, max_features=10000)\n",
    "vectorizer.fit_transform(subsets['train']['text'])\n",
    "texts = {name: vectorizer.transform(subsets[name]['text']) for name in subset_names}\n",
    "categories = {name: subsets[name]['rating'] for name in subset_names}\n",
    "boosted_model = xgb.XGBClassifier(scale_pos_weight=0.1)\n",
    "boosted_model.fit(texts['train'], categories['train'])\n",
    "print('Training metrics')\n",
    "print(classification_report(categories['train'], boosted_model.predict(texts['train'])))\n",
    "print('Development metrics')\n",
    "print(classification_report(categories['dev'], boosted_model.predict(texts['dev'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.99      0.82      2326\n",
      "           1       1.00      0.92      0.96     11597\n",
      "\n",
      "    accuracy                           0.93     13923\n",
      "   macro avg       0.85      0.95      0.89     13923\n",
      "weighted avg       0.95      0.93      0.93     13923\n",
      "\n",
      "Development metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.77      0.65       230\n",
      "           1       0.96      0.91      0.94      1569\n",
      "\n",
      "    accuracy                           0.89      1799\n",
      "   macro avg       0.76      0.84      0.79      1799\n",
      "weighted avg       0.91      0.89      0.90      1799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=norec_tokenizer, stop_words=None, min_df=5, max_df=1000, max_features=5000)\n",
    "vectorizer.fit_transform(subsets['train']['text'])\n",
    "texts = {name: vectorizer.transform(subsets[name]['text']) for name in subset_names}\n",
    "categories = {name: subsets[name]['rating'] for name in subset_names}\n",
    "boosted_model = xgb.XGBClassifier(scale_pos_weight=0.2)\n",
    "boosted_model.fit(texts['train'], categories['train'])\n",
    "print('Training metrics')\n",
    "print(classification_report(categories['train'], boosted_model.predict(texts['train'])))\n",
    "print('Development metrics')\n",
    "print(classification_report(categories['dev'], boosted_model.predict(texts['dev'])))"
   ]
  },
  {
   "source": [
    "Same hyperparameteres as pure XGBoost seems to work best. Tokenizers with CountVectorizer improves on the old version.\n",
    "\n",
    "Time to test with stop word removal"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.nb.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.99      0.83      2326\n",
      "           1       1.00      0.92      0.96     11597\n",
      "\n",
      "    accuracy                           0.93     13923\n",
      "   macro avg       0.86      0.96      0.90     13923\n",
      "weighted avg       0.95      0.93      0.94     13923\n",
      "\n",
      "Development metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.77      0.66       230\n",
      "           1       0.97      0.92      0.94      1569\n",
      "\n",
      "    accuracy                           0.90      1799\n",
      "   macro avg       0.77      0.84      0.80      1799\n",
      "weighted avg       0.92      0.90      0.90      1799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=norec_tokenizer, stop_words=STOP_WORDS, min_df=5, max_df=1000, max_features=10000)\n",
    "vectorizer.fit_transform(subsets['train']['text'])\n",
    "texts = {name: vectorizer.transform(subsets[name]['text']) for name in subset_names}\n",
    "categories = {name: subsets[name]['rating'] for name in subset_names}\n",
    "boosted_model = xgb.XGBClassifier(scale_pos_weight=0.2)\n",
    "boosted_model.fit(texts['train'], categories['train'])\n",
    "print('Training metrics')\n",
    "print(classification_report(categories['train'], boosted_model.predict(texts['train'])))\n",
    "print('Development metrics')\n",
    "print(classification_report(categories['dev'], boosted_model.predict(texts['dev'])))"
   ]
  },
  {
   "source": [
    "Stop word removal is not helpful when using Tokenizers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[11:01:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { num_parallell_tree } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Training metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.70      0.43      2326\n",
      "           1       0.92      0.69      0.79     11597\n",
      "\n",
      "    accuracy                           0.69     13923\n",
      "   macro avg       0.62      0.70      0.61     13923\n",
      "weighted avg       0.82      0.69      0.73     13923\n",
      "\n",
      "Development metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.67      0.41       230\n",
      "           1       0.94      0.77      0.84      1569\n",
      "\n",
      "    accuracy                           0.75      1799\n",
      "   macro avg       0.62      0.72      0.63      1799\n",
      "weighted avg       0.86      0.75      0.79      1799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=norec_tokenizer, stop_words=None, min_df=5, max_df=1000, max_features=10000)\n",
    "vectorizer.fit_transform(subsets['train']['text'])\n",
    "texts = {name: vectorizer.transform(subsets[name]['text']) for name in subset_names}\n",
    "categories = {name: subsets[name]['rating'] for name in subset_names}\n",
    "random_forest_model = xgb.XGBClassifier(\n",
    "    booster='gbtree', \n",
    "    colsample_bynode=0.8,\n",
    "    learning_rate=1,\n",
    "    max_depth=5,\n",
    "    num_parallell_tree=100,\n",
    "    scale_pos_weight=0.2)\n",
    "random_forest_model.fit(texts['train'], categories['train'])\n",
    "print('Training metrics')\n",
    "print(classification_report(categories['train'], boosted_model.predict(texts['train'])))\n",
    "print('Development metrics')\n",
    "print(classification_report(categories['dev'], boosted_model.predict(texts['dev'])))"
   ]
  },
  {
   "source": [
    "And that's enough Random Forest :)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('text-oqa78tdt': venv)",
   "metadata": {
    "interpreter": {
     "hash": "b79a91171a1b654441da0aa02197604a6e053331b5ce31ae0001fbbaddbfb627"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}